{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4\n",
    "\n",
    "В этом задании мы:\n",
    "1. Построим классификатор датасета CIFAR с помощью обычных нейросетей и CNN.\n",
    "2. Поработаем с аугментациями и добьемся большего качества с их помощью.\n",
    "3. Попрактикуемся с техникой fine-tuning: возьмем готовый MobileNet и дообучим последний слой под нашу задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация: CNN против обычных сетей\n",
    "\n",
    "В первой части задания мы повторим то, что проделывалось на лекции.\n",
    "Но на этот раз будем учить сети до победного, пока их качество не перестанет улучшаться - и сравним результаты.\n",
    "\n",
    "Также в конце оценим число параметров в каждой сети, чтобы сравнить эффективность CNN и FC при работе с изображениями.\n",
    "\n",
    "Воспользуемся датасетом CIFAR."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xARvjMxP4JWd",
    "outputId": "8af51d49-953f-4107-f0c6-8318cc64acf2",
    "ExecuteTime": {
     "end_time": "2025-02-05T05:49:29.837605Z",
     "start_time": "2025-02-05T05:48:30.068338Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root=\"./data\", train=False, download=True, transform=ToTensor())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:50<00:00, 3398920.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №1\n",
    "\n",
    "Создайте два объекта `DataLoader` и сохраните их в переменные `train_loader` и `test_loader` (для тренировочной и тестовой выборки соответственно).\n",
    "\n",
    "Используйте размер батча 256."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T05:50:07.780593Z",
     "start_time": "2025-02-05T05:50:07.776061Z"
    }
   },
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №2\n",
    "\n",
    "Обучите полносвязную сеть для классификации CIFAR.\n",
    "\n",
    "Достаточно 3 блоков \"Linear + ReLU\".\n",
    "Ваша задача - вывести accuracy на _тестовой выборке_ на плато.\n",
    "Т.е. нужно обучить сеть настолько долго, чтобы увидеть, как ее качество перестает расти с ростом числа эпох.\n",
    "Для этого попробуйте подвигать `lr` и `num_epochs`.\n",
    "\n",
    "\n",
    "Сдайте в ЛМС предельный accuracy, который может достичь полносвязная сеть."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:24:07.441759Z",
     "start_time": "2025-02-05T11:24:07.427793Z"
    }
   },
   "source": [
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr: float\n",
    "    num_epochs: int\n",
    "\n",
    "\n",
    "def plot_accuracy(epoch: int, values: list[float]):\n",
    "    \"\"\"Пример:\n",
    "\n",
    "    >>> acc.append(validation_accuracy)\n",
    "    >>> plot(i + 1, validation_accuracy)\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.title(\"Epoch %s. Accuracy: %s\" % (epoch, values[-1]))\n",
    "    plt.plot(values)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr: float\n",
    "    num_epochs: int\n",
    "\n",
    "\n",
    "def plot_accuracy(epoch: int, values: list[float]):\n",
    "    \"\"\"Пример:\n",
    "\n",
    "    >>> acc.append(validation_accuracy)\n",
    "    >>> plot(i + 1, validation_accuracy)\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.title(\"Epoch %s. Accuracy: %s\" % (epoch, values[-1]))\n",
    "    plt.plot(values)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    config: TrainConfig,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    # Перенос модели на GPU, если доступен\n",
    "    model = model.to(device)\n",
    "    val_acc = []\n",
    "    # Цикл по эпохам\n",
    "    for i in range(config.num_epochs):\n",
    "        model.train()  # Переключение в режим обучения\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            print(X_batch)\n",
    "            # Перенос батча на GPU\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Обнуление градиентов\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Получение предсказаний модели\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # Расчёт потерь\n",
    "            loss = F.cross_entropy(outputs, y_batch)\n",
    "            loss.backward()  # Обратное распространение ошибки\n",
    "\n",
    "            # Обновление параметров модели\n",
    "            optimizer.step()\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()  # Переключение в режим валидации\n",
    "        total_val_acc = 0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                # Перенос батча на GPU\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                # Получение предсказаний модели\n",
    "                val_outputs = model(X_batch)\n",
    "\n",
    "                # Накопление статистики\n",
    "                total_val_acc += (val_outputs.argmax(1) == y_batch).sum().item()\n",
    "                total_val_samples += X_batch.size(0)\n",
    "\n",
    "        # Расчёт средней потери и точности на валидационном наборе\n",
    "        val_acc.append(total_val_acc / total_val_samples)\n",
    "        plot_accuracy(i + 1, val_acc)\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "# class SimpleFCNModel(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(3 * 32 * 32, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, num_classes),\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "#\n",
    "#\n",
    "# torch.manual_seed(987)\n",
    "# params = TrainConfig(lr=1e-3, num_epochs=50)\n",
    "# model = SimpleFCNModel()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\n",
    "# train_loop(model, train_loader, test_loader, optimizer, params)\n",
    "# Около 0.55 должно выйти"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T06:40:36.715935Z",
     "start_time": "2025-02-05T06:40:36.711419Z"
    }
   },
   "source": [
    "def report_parameters(model: nn.Module):\n",
    "    print(\n",
    "        \"Суммарное количество параметров:\",\n",
    "        sum(p.nelement() for p in model.parameters()),\n",
    "    )\n",
    "    print(\n",
    "        \"Суммарный размер (Мб) параметров:\",\n",
    "        sum(\n",
    "            parameter.nelement() * parameter.element_size()\n",
    "            for parameter in model.parameters()\n",
    "        )\n",
    "        / 1024**2,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T08:11:54.802024Z",
     "start_time": "2025-02-05T08:11:54.797036Z"
    }
   },
   "source": [
    "report_parameters(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Суммарное количество параметров: 1738890\n",
      "Суммарный размер (Мб) параметров: 6.633338928222656\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3\n",
    "\n",
    "Теперь постройте и обучите CNN сеть.\n",
    "Опять же, не используйте глубокую сеть: мы хотим иметь схожее количество параметров для сравнения.\n",
    "\n",
    "Достаточно будет трех блоков \"Conv + ReLU + MaxPool\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(128 * 4 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "torch.manual_seed(987)\n",
    "model = SimpleCNNModel()\n",
    "\n",
    "params = TrainConfig(lr=1e-2, num_epochs=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\n",
    "train_loop(model, train_loader, test_loader, optimizer, params)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.492,\n",
       " 0.5322,\n",
       " 0.5636,\n",
       " 0.5883,\n",
       " 0.6066,\n",
       " 0.6271,\n",
       " 0.6192,\n",
       " 0.6468,\n",
       " 0.6407,\n",
       " 0.6423,\n",
       " 0.6321,\n",
       " 0.6529,\n",
       " 0.6519,\n",
       " 0.6542,\n",
       " 0.6479,\n",
       " 0.6538,\n",
       " 0.6485,\n",
       " 0.6675,\n",
       " 0.6694,\n",
       " 0.6625,\n",
       " 0.6511,\n",
       " 0.6547,\n",
       " 0.6494,\n",
       " 0.6686,\n",
       " 0.6509,\n",
       " 0.6604,\n",
       " 0.6368,\n",
       " 0.6554,\n",
       " 0.665,\n",
       " 0.6739,\n",
       " 0.6696,\n",
       " 0.6693,\n",
       " 0.6466,\n",
       " 0.6279,\n",
       " 0.6543,\n",
       " 0.6418,\n",
       " 0.6591,\n",
       " 0.6662,\n",
       " 0.6654,\n",
       " 0.662,\n",
       " 0.6451,\n",
       " 0.6628,\n",
       " 0.6554,\n",
       " 0.665,\n",
       " 0.6648,\n",
       " 0.6586,\n",
       " 0.6715,\n",
       " 0.6528,\n",
       " 0.6745,\n",
       " 0.6697]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T08:33:56.014723Z",
     "start_time": "2025-02-05T08:33:56.011101Z"
    }
   },
   "source": "report_parameters(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Суммарное количество параметров: 156074\n",
      "Суммарный размер (Мб) параметров: 0.5953750610351562\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на качество и на число параметров.\n",
    "Качество получается выше, а число параметров - на порядок меньше.\n",
    "\n",
    "Делаем вывод, что CNN позволяют выбивать лучшее качество, чем обычные сети, и при меньшем числе параметров.\n",
    "\n",
    "Но CNN - не единственный способ улучшить качество при работе с картинками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №4\n",
    "Реализуйте следующие аугментации:\n",
    "1. Горизонтальное отражение (Horizontal Flip) с вероятностью применения 30%\n",
    "2. Вращение на угол (Rotate), близкий к 30 градусам, с вероятностью применения около 30%.\n",
    "3. Random Resized Crop - тут выберите нужные параметры самостоятельно.\n",
    "4. Normalize. Нормализовать нужно вдоль трех осей изображения. Среднее и std подсчитайте самостоятельно, используя `train_dataset` (в подсчет статистик _нельзя_ включать `test_dataset`).\n",
    "\n",
    "Используйте библиотеку `albumentations`.\n",
    "Не забудьте, что `albumentations` работает с numpy-массивами.\n",
    "Придется перегонять данные из pytorch в numpy-массивы и обратно:\n",
    "\n",
    "```python\n",
    "np_array = tensor.numpy()\n",
    "tensor_back = torch.from_numpy(np_array)\n",
    "```\n",
    "\n",
    "Сохраните аугментации в переменную `transforms` и сдайте свой код в ЛМС.\n",
    "\n",
    "<details>\n",
    "<summary>Как ваш код будет проверяться</summary>\n",
    "\n",
    "```python\n",
    "import albumentations as A\n",
    "\n",
    "# <Ваш код здесь>\n",
    "\n",
    "# Затем проверки на переменную transforms\n",
    "assert some_check(transforms)\n",
    "assert another_check(transforms)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:32:41.759387Z",
     "start_time": "2025-02-05T09:32:36.281830Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install albumentations==1.4.2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==1.4.2 in c:\\python311\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (1.13.1)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (0.25.1)\n",
      "Requirement already satisfied: PyYAML in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (4.12.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (1.6.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0 in c:\\python311\\lib\\site-packages (from albumentations==1.4.2) (4.11.0.86)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations==1.4.2) (3.2.1)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations==1.4.2) (10.3.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations==1.4.2) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations==1.4.2) (2025.1.10)\n",
      "Requirement already satisfied: packaging>=21 in c:\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations==1.4.2) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations==1.4.2) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python311\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations==1.4.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python311\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations==1.4.2) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f4YzCLbyXcks",
    "ExecuteTime": {
     "end_time": "2025-02-05T10:49:02.803647Z",
     "start_time": "2025-02-05T10:49:02.795646Z"
    }
   },
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "image_size = 32\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.Rotate(limit=30, p=0.3),\n",
    "        A.RandomResizedCrop((image_size, image_size), scale=(0.8, 1.0), p=0.3),\n",
    "        A.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.24703279, 0.24348423, 0.26158753],\n",
    "            max_pixel_value=1.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментации\n",
    "Зачастую аугментации помогают увеличить качество модели.\n",
    "Объясняется это так: аугментация изображений обогащает датасет новыми картинками, сгенерированными из существующих.\n",
    "Переобучения не происходит, потому что мы не просто дублируем изображения, а немного изменяем их.\n",
    "### Задание №5\n",
    "\n",
    "Обучите CNN с использованием аугментаций.\n",
    "Как и в прошлых заданиях, держите обучение до конца - пока loss не выйдет на плато.\n",
    "\n",
    "Ваша задача - получить accuracy выше 76%.\n",
    "Сдайте в ЛМС:\n",
    "- код класса модели. Класс должен называться `SimpleCNNModel`;\n",
    "- .pt файл с обученной моделью;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T12:40:42.126981Z",
     "start_time": "2025-02-05T12:40:41.965860Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DatasetWithTransforms(Dataset):\n",
    "    def __init__(self, original_dataset: Dataset, transforms: A.Compose):\n",
    "        super().__init__()\n",
    "        self._transforms = transforms\n",
    "        self._dataset = original_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        print('###', self._dataset)\n",
    "        img, label = self._dataset[index]\n",
    "        img_as_np = img.numpy()\n",
    "        # albumentations принимает картинку в формате (w, h, c), но pytorch хранит в (c, w, h)\n",
    "        # np_array.transpose(1, 2, 0) сделает так, чтобы сначала шла ось 1, потом ось 2, потом ось 0\n",
    "        # т.е. (c, w, h) перейдет в (w, h, c)\n",
    "        #       0  1  2              1  2  0\n",
    "        transformed = self._transforms(image=img_as_np.transpose(1, 2, 0))[\"image\"]\n",
    "        print('@@@', type(img_as_np))\n",
    "        return transformed, label\n",
    "\n",
    "\n",
    "train_dataset_augs = DatasetWithTransforms(train_dataset, transforms)\n",
    "train_loader_augs = DataLoader(\n",
    "    train_dataset_augs, batch_size=256, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "# Надо не забыть в тесте нормировать картинки на те же статистики, что в train датасете\n",
    "transforms_test = A.Compose(\n",
    "    [\n",
    "        A.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.24703279, 0.24348423, 0.26158753],\n",
    "            max_pixel_value=1.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "test_dataset_augs = DatasetWithTransforms(test_dataset, transforms_test)\n",
    "test_loader_augs = DataLoader(\n",
    "    test_dataset_augs, batch_size=256, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "torch.manual_seed(seed=987)\n",
    "model = SimpleCNNModel()\n",
    "\n",
    "params = TrainConfig(lr=1e-3, num_epochs=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\n",
    "x, y = next(iter(train_dataset))\n",
    "print(x)\n",
    "train_loop(model, train_loader_augs, test_loader_augs, optimizer, params)\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image must be numpy array type",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 51\u001B[0m\n\u001B[0;32m     49\u001B[0m params \u001B[38;5;241m=\u001B[39m TrainConfig(lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m     50\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mparams\u001B[38;5;241m.\u001B[39mlr)\n\u001B[1;32m---> 51\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(x)\n\u001B[0;32m     53\u001B[0m train_loop(model, train_loader_augs, test_loader_augs, optimizer, params)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001B[0m, in \u001B[0;36mCIFAR10.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    116\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 119\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "Cell \u001B[1;32mIn[43], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(img)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m CIFAR10(root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m'\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m img: \u001B[43mtransforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      2\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m987\u001B[39m)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\albumentations\\core\\composition.py:475\u001B[0m, in \u001B[0;36m__call__\u001B[1;34m(self, force_apply, *args, **data)\u001B[0m\n\u001B[0;32m    472\u001B[0m     augs \u001B[38;5;241m=\u001B[39m ReplayCompose\u001B[38;5;241m.\u001B[39m_restore_for_replay(saved_augmentations)\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m augs(force_apply\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 475\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_restore_for_replay\u001B[39m(\n\u001B[0;32m    477\u001B[0m     transform_dict: Dict[\u001B[38;5;28mstr\u001B[39m, Any], lambda_transforms: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    478\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m TransformType:\n\u001B[0;32m    479\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Args:\u001B[39;00m\n\u001B[0;32m    480\u001B[0m \u001B[38;5;124;03m    lambda_transforms (dict): A dictionary that contains lambda transforms, that\u001B[39;00m\n\u001B[0;32m    481\u001B[0m \u001B[38;5;124;03m    is instances of the Lambda class.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m \n\u001B[0;32m    486\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    487\u001B[0m     applied \u001B[38;5;241m=\u001B[39m transform_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplied\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\albumentations\\core\\composition.py:498\u001B[0m, in \u001B[0;36mpreprocess\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m SERIALIZABLE_REGISTRY[name]\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransforms\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m args:\n\u001B[0;32m    497\u001B[0m         args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransforms\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 498\u001B[0m             ReplayCompose\u001B[38;5;241m.\u001B[39m_restore_for_replay(t, lambda_transforms\u001B[38;5;241m=\u001B[39mlambda_transforms)\n\u001B[0;32m    499\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransforms\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    500\u001B[0m         ]\n\u001B[0;32m    501\u001B[0m     transform \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m    503\u001B[0m transform \u001B[38;5;241m=\u001B[39m cast(BasicTransform, transform)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\albumentations\\core\\composition.py:735\u001B[0m, in \u001B[0;36m_get_data_shape\u001B[1;34m(self, data_name, internal_name, data)\u001B[0m\n\u001B[0;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[1;31mTypeError\u001B[0m: image must be numpy array type"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментации улучшили качество.\n",
    "\n",
    "Заметьте, что нормализацию можно было бы применить и в прошлом пункте, чтобы более честно оценить, какой прирост дали развороты и вращения изображения.\n",
    "Советуем самостоятельно провести эксперимент и увидеть различия.\n",
    "\n",
    "<details>\n",
    "    <summary>Какие результаты ожидать</summary>\n",
    "    У авторов получилось около 73% accuracy при использовании только лишь нормализации. При добавлении остальных аугментаций качество было еще выше.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "### Задание №6\n",
    "Transfer learning состоит в том, чтобы взять готовую сеть и дообучить небольшую ее часть.\n",
    "В этом задании мы будем учить FC слой в конце MobileNet.\n",
    "\n",
    "Загрузите предварительно обученную модель из серии `MobileNet`, используйте `MobileNet_V3_large`.\n",
    "\n",
    "Поменяйте ее последний слой (классификатор) на один линейный слой.\n",
    "Обучите все это дело, меняя **только** параметры своего слоя (подумайте, что передавать в оптимизатор).\n",
    "Сохраните обученный слой (и только его) в `model_finetune.pt`.\n",
    "\n",
    "Сдайте в ЛМС .pt файл и код, создающий вашу модель в переменную `model_finetune`.\n",
    "Чтобы сдать это задание, достаточно набрать accuracy > 40%."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T12:52:33.840295Z",
     "start_time": "2025-02-05T12:47:55.216237Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms\n",
    "\n",
    "model = models.mobilenet_v3_large(pretrained=True)\n",
    "model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 10)\n",
    "\n",
    "# Заморозка всех слоев, кроме последнего\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # MobileNet ожидает изображения 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Определение оптимизатора (только для последнего слоя)\n",
    "optimizer = optim.Adam(model.classifier[-1].parameters(), lr=1e-3)\n",
    "\n",
    "# Определение функции потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучение модели\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Валидация модели\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
    "\n",
    "# Сохранение только последнего слоя\n",
    "torch.save(model.classifier[-1].state_dict(), \"model_finetune.pt\")\n",
    "\n",
    "# Переменная для сдачи в LMS\n",
    "model_finetune = model.classifier[-1]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 782/782 [00:51<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 782/782 [00:50<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 782/782 [00:51<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.5251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 782/782 [00:52<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.5168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 782/782 [00:51<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.5079\n",
      "Accuracy on test set: 82.27%\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Суммарное количество параметров:\",\n",
    "    sum(p.nelement() for p in trainable_params),\n",
    ")\n",
    "print(\n",
    "    \"Суммарный размер (Мб) параметров:\",\n",
    "    sum(\n",
    "        parameter.nelement() * parameter.element_size()\n",
    "        for parameter in trainable_params\n",
    "    )\n",
    "    / 1024**2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество, возможно, просело, зато учим намного меньше параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №7\n",
    "Возьмите предпоследний слой вашей CNN модели (тот, что до классификатора).\n",
    "Этот слой выдает вектора.\n",
    "\n",
    "Возьмите любой объект из класса 0, подсчитайте его косинусную схождесть со всеми остальными объектами из класса 0, усредните.\n",
    "Затем подсчитайте то же число, только против всех объектов из класса 1, тоже усредните.\n",
    "Отправьте в ЛМС два числа, разделенные запятой. Например, \"1, 1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Небольшой бонус\n",
    "Эмбеддинги можно визуализировать, используя t-SNE.\n",
    "Посмотрите, что получается, попробуйте объяснить картину.\n",
    "\n",
    "Если модель достаточно качественная, то схожие классы должны собираться в одну кучку, при этом у кучек должны прослеживаться границы.\n",
    "Конечно же, не забывайте про выбросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "result = torch.empty((0, 2048))\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    embedding_model.cpu()\n",
    "    for x_batch, y_batch in test_loader_augs:\n",
    "        embedding = embedding_model(x_batch).flatten(1)\n",
    "        embedding /= embedding.norm()\n",
    "        result = torch.concat((result, embedding))\n",
    "        labels.extend(y_batch.tolist())\n",
    "\n",
    "tsne = TSNE(random_state=42)\n",
    "plot_data = tsne.fit_transform(result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "scatter = ax.scatter(\n",
    "    plot_data[:, 0],\n",
    "    plot_data[:, 1],\n",
    "    c=labels,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"k\",\n",
    "    s=20,\n",
    "    alpha=1,\n",
    ")\n",
    "plt.colorbar(scatter)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "start-dl-ph78tHa0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
